{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Matyáš Sládek <br>\n",
    "Rok: 2020 <br>\n",
    "\n",
    "Tento soubor slouží k optimalizaci parametrů klasifikačních algoritmů pomocí grid GridSampler a TPESampler z knihovny Optuna. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka importuje potřebné knihovny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hpoptimise_optuna import HPoptimise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka obsahuje funkci pro načtení a zpracování potřebných dat a spuštění optimalizace parametrů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise(optimised_params, dataset, library, classifiers, parameters):\n",
    "    \"\"\"\n",
    "    Optimises hyper parameters of classifiers on given features.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    optimised_params:       Dictionary of optimised hyper parameters\n",
    "    dataset:                Name of a dataset to indicate which extracted features to load (dataset from which the features were extracted)\n",
    "    library:                Name of a library to indicate which extracted features to load (library with which the features were extracted)\n",
    "    optimised_feature_sets: Dictionary containing optimised feature sets\n",
    "    classifiers:            Dictionary containing classifiers to be optimised\n",
    "    max_evals:              Maximum number of optimisation iterations\n",
    "    verbose:                Indicates what information about optimisation to print out\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_best_params():\n",
    "        \"\"\"\n",
    "        Optimises hyper parameters of a given classifier.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Add the feature set name to the optimised parameters dictionary if not there already\n",
    "        if not feature_set_name in optimised_params:\n",
    "            optimised_params[feature_set_name] = {}\n",
    "                 \n",
    "        # Initialise the optimiser\n",
    "        estim = HPoptimise(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, classifier_name=classifier_name, classifier=classifier, filepath='../metadata/optuna_studies/{}_{}_{}_{}_{}.pkl'.format(dataset, library, feature_set_name, classifier_name, validation_type), other_params=parameters)   # Initialize optimiser object\n",
    "        \n",
    "        print('Optimising \\033[1m{}\\033[0m classifier hyper parameters on feature set \\033[1m{}\\033[0m:'.format(classifier_name, feature_set_name))\n",
    "        \n",
    "        # Store the start time of the optimisation of current classifier\n",
    "        t_start = time.time()\n",
    "        \n",
    "        # Start the optimisation process\n",
    "        with warnings.catch_warnings():   # Suppress warnings regarding experimental features\n",
    "            warnings.simplefilter('ignore', category=ExperimentalWarning)\n",
    "            estim.optimise()\n",
    "        \n",
    "        # Add the classifier name to the optimised parameters dictionary if not there already\n",
    "        if not classifier_name in optimised_params[feature_set_name]:\n",
    "            optimised_params[feature_set_name][classifier_name] = {}\n",
    "            \n",
    "        # Store the best parameters for selected validation type\n",
    "        optimised_params[feature_set_name][classifier_name][validation_type] = estim.best_params()\n",
    "\n",
    "        print('\\nOptimisation finished in ~\\033[1m{}\\033[0m.'.format(str(timedelta(seconds=(time.time() - t_start))).split(\".\")[0]))\n",
    "#         print('Cross-validation score: \\033[1m{}\\033[0m'.format(1 - current_loss))\n",
    "        print('Best params: \\033[1m{}\\033[0m'.format(estim.best_params()))\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimised_feature_sets_names = []   # Stores the names of optimised feature sets to be used for hyper parameter optimisation\n",
    "    validation_type = 'CV' if parameters['use_cross_validation'] else 'VS'   # Stores the shortcut for the type of selected hyper parameter validation method\n",
    "    X_val = None   # Stores the features of validation set if used\n",
    "    y_val = None   # Stores the genres of validation set if used\n",
    "    \n",
    "    # If any of the optimised feature sets is selected\n",
    "    if parameters['use_opt_feature_set_FS_CV'] or parameters['use_opt_feature_set_FS_VS'] or parameters['use_opt_feature_set_BE_CV'] or parameters['use_opt_feature_set_BE_VS']:\n",
    "        \n",
    "        # Load optimised feature sets if available\n",
    "        try:\n",
    "            with open('../metadata/misc/optimised_feature_sets.json') as f:\n",
    "                optimised_feature_sets = json.load(f)   \n",
    "        except Exception as e:\n",
    "            print('Failed to read file: \"../metadata/misc/optimised_feature_sets.json\"!', file=sys.stderr)\n",
    "            print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "            return -1\n",
    "        \n",
    "        # If feature set optimised with forward selection and cross-validation is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_FS_CV']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_FS_CV')\n",
    "            \n",
    "        # If feature set optimised with forward selection and validation set is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_FS_VS']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_FS_VS')\n",
    "            \n",
    "        # If feature set optimised with backward elimination and cross-validation is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_BE_CV']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_BE_CV')\n",
    "            \n",
    "        # If feature set optimised with backward elimination and validation set is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_BE_VS']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_BE_VS')\n",
    "    \n",
    "    # Load specified extracted features\n",
    "    try:                \n",
    "        features = pd.read_csv('../metadata/features/features_{}_{}.csv'.format(dataset, library), index_col=0, header=[0, 1, 2])        \n",
    "    except Exception as e:\n",
    "        print('Failed to read file: \"../metadata/features/features_{}_{}.csv\"!'.format(dataset, library), file=sys.stderr)\n",
    "        print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "\n",
    "    # Perform One-hot encoding on categorical features\n",
    "    for column in features.select_dtypes(include='object'):   # For each categorical column\n",
    "        dummy_columns = pd.get_dummies(features[column], drop_first=True)   # Encode the column values  \n",
    "        features = features.drop(columns=column)   # Drop the column from the dataframe\n",
    "        dummy_columns.columns = pd.MultiIndex.from_product([[column[0]], [column[1]], ['{}'.format(c) for c in dummy_columns.columns]], names=features.columns.names) # Reindex for consistance\n",
    "        features = pd.concat([features, dummy_columns], axis=1).sort_index(axis=1)   # Append columns to features dataframe\n",
    "        \n",
    "    feature_names = list(features.columns.levels[0])   # Get names of all features\n",
    "    feature_sets = {}   # Stores non-optimised feature sets\n",
    "    \n",
    "    # If all features is selected, add all feature names to the dictionary of feature sets to be used\n",
    "    if parameters['use_all_features']:\n",
    "        feature_sets['all'] = feature_names\n",
    "    \n",
    "    # Load track-genre list\n",
    "    try:                \n",
    "        genres = pd.read_csv(\"../metadata/track_genre_lists/{}_track_genre_list.csv\".format(dataset), index_col=0, header=0)        \n",
    "    except Exception as e:\n",
    "        print('Failed to read file: \"../metadata/track_genre_lists/{}_track_genre_list.csv\"!'.format(dataset), file=sys.stderr)\n",
    "        print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "    \n",
    "    genres = genres.loc[features.index]   # Remove unwanted data from track-genre list (data about tracks removed from features because of corruption or other reason)\n",
    "    \n",
    "    # Encode genre labels\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(np.ravel(genres))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # For each of the non-optimised feature sets\n",
    "    for feature_set_name, feature_set in feature_sets.items():\n",
    "        X = features[feature_set].values   # Extract selected feature set values to ndarray\n",
    "        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)   # Split the samples to train and test data (test data is not used)\n",
    "                \n",
    "        if parameters['use_cross_validation']:   # If cross-validation is selected, transform all the training data                     \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "        else:   # If validation set is selected, split the training data to training and validation sets\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=parameters['validation_set_size'], random_state=42, stratify=y_train)\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_val = scaler.transform(X_val)\n",
    "\n",
    "        # For each of the selected classifiers, perform hyper parameter optimisation\n",
    "        for classifier_name, classifier in classifiers.items():                            \n",
    "            get_best_params()\n",
    "            print('-'*100)\n",
    "            \n",
    "    # For each of the optimised feature sets\n",
    "    for feature_set_name in optimised_feature_sets_names:\n",
    "        \n",
    "        # For each of the selected classifiers\n",
    "        for classifier_name, classifier in classifiers.items():\n",
    "            \n",
    "            # Get the optimised feature set\n",
    "            try:\n",
    "                feature_set = optimised_feature_sets[dataset][library][classifier_name][feature_set_name]\n",
    "            except KeyError:\n",
    "                print('Optimised feature set {} for classifier {} not found!'.format(feature_set_name, classifier_name), file=sys.stderr)\n",
    "                continue\n",
    "            \n",
    "            X = features[feature_set].values   # Get the features of the features set\n",
    "            X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)   # Split the samples to train and test data (test data is not used)\n",
    "            \n",
    "            if parameters['use_cross_validation']:   # If cross-validation is selected, transform all the training data                         \n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "            else:   # If validation set is selected, split the training data to training and validation sets\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=parameters['validation_set_size'], random_state=42, stratify=y_train)\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_val = scaler.transform(X_val)\n",
    "            \n",
    "            # Perform hyper parameter optimisation\n",
    "            get_best_params()\n",
    "            print('-'*100)\n",
    " \n",
    "                                   \n",
    "    return optimised_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka slouží k nastavení potřebných parametrů a výběru datových sad, sad atributů a klasifikátorů, pro které má být optimalizace provedena. <br>\n",
    "Detailní popis optimalizovaných parametrů je možné pro klasifikátor XGBClassifier nalézt zde <https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn> a pro klasifikátory z knihovny Sci-kit learn zde <https://scikit-learn.org/stable/>. <br>\n",
    "Pro jaké datové sady má být optimalizace provedena je možné zvolit v proměnné <code>datasets</code> odkomentováním/zakomentováním příslušných záznamů. <br>\n",
    "Při použití vlastní datové sady je nutné do proměnné <code>datasets</code> přidat stejný název datové sady, jako byl použit pro extrakci atributů v souboru <strong>feature_extraction.ipynb</strong> <br>\n",
    "Dále je nutné zvolit, jestli optimalizaci provést na atributech extrahovaných pomocí knihovny Librosa či Essentia odkomentováním/zakomentováním příslušných záznamů v proměnné <code>feature_extraction_libraries</code>. <br>\n",
    "Pro které klasifikátory má být optimalizace provedena je možné zvolit v proměnné <code>classifiers</code> odkomentováním/zakomentováním příslušných záznamů. <br>\n",
    "Při použití jiného klasifikátorů je tento nutné přidat do proměnné <code>classifiers</code> ve formátu {zvolený_název_klasifikátoru}:{odkaz_na_objekt_klasifikátoru}. <br>\n",
    "Paremetry klasifikátorů je možné upravit v proměnné <code>default_params</code> v souboru <strong>hpoptimise_optuna.py</strong>, případně pro nový klasifikátor přidat záznam ve formátu {zvolený_název_klasifikátoru}:{slovník_parametrů}, kde zvolený název klasifikátoru musí odpovídat názvu v proměnné <code>classifiers</code>. <br>\n",
    "Dále je pro případný nový klasifikátor přidat všechny náležitosti (optimalizační funkce atd.) dle konvencí v souboru <strong>hpoptimise_optuna.py</strong>. <br>\n",
    "V proměnné <code>parameters</code> je možné nastavit průběh optimalizace. <br>\n",
    "<br>\n",
    "Popis parametrů: <br>\n",
    "<ul>\n",
    "    <li><code>max_trials</code> Hodnota značí maximální počet iterací algoritmu TPESampler, u algoritmu GridSampler ignorováno</li>\n",
    "    <li><code>max_trials_no_change</code> Hodnota značí maximální počet iterací algoritmu TPESampler bez navášení skóre, po kterých bude optimalizace zastavena, u algoritmu GridSampler ignorováno</li>\n",
    "    <li><code>use_cross_validation</code> Hodnota True znamená použití křížové validace, hodnota False pak validace na validační sadě</li>\n",
    "    <li><code>cross_validation_num_of_folds</code> Značí, kolikanásobná bude křížová validace, pokud je tato metoda zvolena</li>\n",
    "    <li><code>validation_set_size</code> Značí, jaká část trénovacích dat bude vyhrazena jako validační, pokud je tato metoda zvolena</li>\n",
    "    <li><code>use_all_features</code> Hodnota True znamená použití celé sady atributů</li>\n",
    "    <li><code>use_opt_feature_set_FS_CV</code> Hodnota True znamená použití sady atributů vybrané pomocí metody dopředné selekce a křížové validace</li>\n",
    "    <li><code>use_opt_feature_set_FS_VS</code> Hodnota True znamená použití sady atributů vybrané pomocí metody dopředné selekce a validace na validační saďe</li>\n",
    "    <li><code>use_opt_feature_set_BE_CV</code> Hodnota True znamená použití sady atributů vybrané pomocí metody zpětné eliminace a křížové validace</li>\n",
    "    <li><code>use_opt_feature_set_BE_VS</code> Hodnota True znamená použití sady atributů vybrané pomocí metody zpětné eliminace a validace na validační saďe</li>\n",
    "</ul>\n",
    "\n",
    "Klasifikátory <code>MLPClassifier</code>, <code>DecisionTreeClassifier</code>, <code>RandomForestClassifier</code> a <code>XGBClassifier</code> používají algoritmus TPESampler a klasifikátory <code>LogisticRegression</code>, <code>KNeighborsClassifier</code>, <code>SVC_linear</code> a <code>SVC_rbf</code> používají algoritmus GridSampler. Stejně jako u selekce atributů je při nastavování parametrů třeba brát v potaz, že optimalizace je velmi výpočetně náročný proces, obzvláště pak v případě optimalizace algoritmů <code>RandomForestClassifier</code> a <code>XGBClassifier</code>.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    # List of datasets whose features should be used, unwanted can be commented out   \n",
    "    datasets = [\n",
    "#         'EBD',\n",
    "#         'FMA',\n",
    "        'GTZAN'\n",
    "    ]\n",
    "    \n",
    "    # List of feature extraction libraries whose features should be used, unwanted can be commented out   \n",
    "    feature_extraction_libraries = [\n",
    "#         'librosa',\n",
    "        'essentia'\n",
    "    ]\n",
    "    \n",
    "    # Dictionary of classifier names and objects which hyper parameters should be optimised, unwanted can be commented out   \n",
    "    classifiers = {\n",
    "        # sklearn classifiers\n",
    "#         'LogisticRegression':LogisticRegression,\n",
    "        'KNeighborsClassifier':KNeighborsClassifier,\n",
    "#         'MLPClassifier':MLPClassifier,\n",
    "#         'DecisionTreeClassifier':DecisionTreeClassifier,\n",
    "#         'SVC_linear':SVC,\n",
    "#         'SVC_rbf':SVC,\n",
    "        \n",
    "        # sklearn ensemble classifiers\n",
    "#         'RandomForestClassifier':RandomForestClassifier,\n",
    "        \n",
    "        # other classifiers\n",
    "#         'XGBClassifier':XGBClassifier,\n",
    "    }\n",
    "    \n",
    "    parameters = {\n",
    "        'max_trials':1000,   # Determines the maximum number of optimisation iterations\n",
    "        'max_trials_no_change':100,   # Determines the maximum number of optimisation iterations with no improvement of score, after which the optimisation will be stopped\n",
    "        'use_cross_validation': False,   # Set to true to use cross-validation on the entire training data, else a portion of the train data will be reserved as validation set and cross-validation will not be used\n",
    "        'cross_validation_num_of_folds' : 5,   # Determines the number of cross-validation folds, ignored if 'use_cross_validation' parameter is set to False\n",
    "        'validation_set_size': 0.2,   # Determines the size of the portion of training data, which will be reserved as validation set, ignored if 'use_cross_validation' parameter is set to True\n",
    "        'use_all_features': True,   # Use all available features extracted with selected library\n",
    "        'use_opt_feature_set_FS_CV': False,   # Set this to true to use feature set optimised with forward selection and cross-validation\n",
    "        'use_opt_feature_set_FS_VS': True,   # Set this to true to use feature set optimised with backward elimination and validation set\n",
    "        'use_opt_feature_set_BE_CV': False,   # Set this to true to use feature set optimised with forward selection and cross-validation\n",
    "        'use_opt_feature_set_BE_VS': True,   # Set this to true to use feature set optimised with backward elimination and validation set\n",
    "    }\n",
    "\n",
    "    # Load optimised hyper parameters file if available\n",
    "    try:\n",
    "        with open('../metadata/misc/optimised_hyper_parameters.json') as f:\n",
    "            optimised_params = json.load(f)            \n",
    "    except FileNotFoundError:    \n",
    "        optimised_params = {}\n",
    "    \n",
    "    if parameters['use_cross_validation']:\n",
    "        print('Using cross-validation.')\n",
    "    else:\n",
    "        print('Using validation set.')\n",
    "    \n",
    "    t_start = time.time() # Store the start time of the hyper parameter optimisation\n",
    "        \n",
    "    for dataset in datasets:   # For each of the selected datasets\n",
    "        \n",
    "        # Add dataset name to optimised hyper parameters dictionary if not there already\n",
    "        if not dataset in optimised_params:\n",
    "            optimised_params[dataset] = {}\n",
    "        \n",
    "        for library in feature_extraction_libraries:   # For each of the selected extraction libraries\n",
    "            \n",
    "            # Add extraction library name to optimised hyper parameters dictionary if not there already\n",
    "            if not library in optimised_params[dataset]:\n",
    "                optimised_params[dataset][library] = {}\n",
    "            \n",
    "            print('#'*100)\n",
    "            print('Optimising hyper parameters on \\033[1m{}\\033[0m dataset features extracted with \\033[1m{}\\033[0m library:\\n'.format(dataset, library))\n",
    "            t = time.time()   # Store the start time of hyper parameter optimisation for selected dataset and extraction library features\n",
    "            optimise(optimised_params[dataset][library], dataset, library, classifiers, parameters)   # Start the optimisation\n",
    "            print(\"\\nClassifiers hyper parameter optimisation on \\033[1m{}\\033[0m dataset features extracted with \\033[1m{}\\033[0m library finished in ~\\033[1m{}\\033[0m.\".format(dataset, library, str(timedelta(seconds=(time.time() - t))).split(\".\")[0]))\n",
    "            print('#'*100)\n",
    "            \n",
    "            # Save optimised hyper parameters to .json file\n",
    "            with open('../metadata/misc/optimised_hyper_parameters.json', 'w') as json_file:\n",
    "                json.dump(optimised_params, json_file)\n",
    "    \n",
    "    print(\"Classifiers hyper parameter optimisation on all selected datasets and all selected features finished in ~\\033[1m{}\\033[0m.\".format(str(timedelta(seconds=(time.time() - t_start))).split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
