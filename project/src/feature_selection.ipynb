{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Matyáš Sládek <br>\n",
    "Rok: 2020 <br>\n",
    "\n",
    "Tento soubor slouží k selekci atributů pomocí metod dopředné selekce a zpětné eliminace. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka importuje potřebné knihovny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tqdm.notebook import tqdm   # Progress bars\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka obsahuje funkci pro tisk na obrazovku a zároveň do souboru <strong>feature_selection.out</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Prints to stdout and logs to file.\n",
    "    \"\"\"\n",
    "    print(*args, **kwargs)   # Print to stdout\n",
    "    with open('../metadata/misc/feature_selection.out','a') as file:\n",
    "        print(re.sub('\\\\033\\[.m', '', *args), file=file)   # Print to log file with removed bold text, that would not display correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka obsahuje funkci pro načtení a zpracování dat a provedení selekce atributů pomocí metody dopředné selekce či zpětné eliminace s použitím křížové validace či validace na validační sadě. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_sets(optimised_feature_sets, dataset, library, classifiers, default_params, parameters):\n",
    "    \"\"\"\n",
    "    Finds optimal feature subset using forward selection and/or backward elimination method.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    optimised_feature_sets: Dictionary containing optimised feature sets\n",
    "    dataset:                Name of a dataset to indicate which extracted features to load (dataset from which the features were extracted)\n",
    "    library:                Name of a library to indicate which extracted features to load (library with which the features were extracted)\n",
    "    classifiers:            Dictionary containing classifiers for which the optimal feature subset should be found\n",
    "    default_params:         Dictionary containing parameters for classifiers\n",
    "    parameters:             Parameters about which selection to use etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of CPU cores for multiprocessing\n",
    "    num_cpus = 1 if os.cpu_count() is None else os.cpu_count()\n",
    "    \n",
    "    # Sets a character for formatting printing\n",
    "    format_char = ''\n",
    "    if parameters['verbose']:\n",
    "        format_char = '\\n'\n",
    "      \n",
    "    # Load specified extracted features\n",
    "    try:                \n",
    "        features = pd.read_csv('../metadata/features/features_{}_{}.csv'.format(dataset, library), index_col=0, header=[0, 1, 2])        \n",
    "    except Exception as e:\n",
    "        print_log('Failed to read file: \"../metadata/features/features_{}_{}.csv\"!'.format(dataset, library), file=sys.stderr)\n",
    "        print_log('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "\n",
    "    # Perform One-hot encoding on categorical features\n",
    "    for column in features.select_dtypes(include='object'):   # For each categorical column\n",
    "        dummy_columns = pd.get_dummies(features[column], drop_first=True)   # Encode the column values  \n",
    "        features = features.drop(columns=column)   # Drop the column from the dataframe\n",
    "        dummy_columns.columns = pd.MultiIndex.from_product([[column[0]], [column[1]], ['{}'.format(c) for c in dummy_columns.columns]], names=features.columns.names) # Reindex for consistance\n",
    "        features = pd.concat([features, dummy_columns], axis=1).sort_index(axis=1)   # Append columns to features dataframe\n",
    "\n",
    "    feature_names = list(features.columns.levels[0])   # Get names of all features     \n",
    "        \n",
    "    # Load track-genre list\n",
    "    try:                \n",
    "        genres = pd.read_csv(\"../metadata/track_genre_lists/{}_track_genre_list.csv\".format(dataset), index_col=0, header=0)        \n",
    "    except Exception as e:\n",
    "        print_log('Failed to read file: \"../metadata/track_genre_lists/{}_track_genre_list.csv\"!'.format(dataset), file=sys.stderr)\n",
    "        print_log('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "    \n",
    "    genres = genres.loc[features.index]   # Remove unwanted data from track-genre list (data about tracks removed from features because of corruption or other reason)\n",
    "\n",
    "    # Encode genre labels\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(np.ravel(genres))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "    # FORWARD SELECTION\n",
    "    \n",
    "    if parameters['use_forward_selection']:   # If forward selection is selected\n",
    "        print_log('Selecting features using forward selection method:')\n",
    "        t1 = time.time()   # Store the start time of forward selection \n",
    "\n",
    "        for classifier_name, classifier in tqdm(classifiers.items(), desc='classifiers', leave=True):   # For each of the selected classifiers\n",
    "            print_log('\\nSelecting features for classifier \\033[1m{}\\033[0m:\\n'.format(classifier_name))\n",
    "            t2 = time.time()   # Store the start time of forward selection for the classifier\n",
    "\n",
    "            classifier = classifier(**default_params[classifier_name])   # Initialize the classifier\n",
    "            n_jobs = parameters['cross_validation_num_of_folds'] if classifier_name in ['LogisticRegression', 'DecisionTreeClassifier', 'SVC_linear', 'SVC_rbf'] else 1   # Set number of processes for cross-validation for classifiers which do not support multiprocessing\n",
    "            n_jobs = n_jobs if n_jobs <= num_cpus else num_cpus   # Limit number of processes to CPU cores\n",
    "            remaining_features = feature_names.copy()   # Stores the feature names from which to select\n",
    "            best_features = []   # Stores the best selected features\n",
    "            score_max_previous = 0   # Stores the classification score of the best feature set in last completed iteration\n",
    "\n",
    "            for i in range(len(feature_names)):   # For each feature name\n",
    "                scores_tmp = pd.Series(index=remaining_features, dtype='float64')   # Stores scores for all feature sets in each iteration\n",
    "\n",
    "                for feature_name in remaining_features:   # For each of the yet unselected features\n",
    "                    X = features[best_features + [feature_name]].values   # Add the feature to the best feature set\n",
    "                    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)   # Split the samples to train and test data (test data is not used)\n",
    "                    \n",
    "                    if parameters['use_cross_validation']:   # If cross-validation is selected, transform all the training data and perform cross-validation                        \n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        scores_tmp[feature_name] = cross_val_score(classifier, X_train, y_train, cv=StratifiedKFold(n_splits=parameters['cross_validation_num_of_folds']), n_jobs=n_jobs).mean()\n",
    "                    else:   # If validation set is selected, split the training data to training and validation sets, fit the classifier and perform classification on the validation set\n",
    "                        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=parameters['validation_set_size'], random_state=42, stratify=y_train)\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        X_val = scaler.transform(X_val)\n",
    "                        classifier.fit(X_train, y_train)\n",
    "                        scores_tmp[feature_name] = classifier.score(X_val, y_val)\n",
    "                        \n",
    "                    if parameters['verbose']:   # If verbose is selected, print out score for each of the added features\n",
    "                        print_log('Score with feature \\033[1m{}\\033[0m: \\033[1m{}\\033[0m'.format(feature_name, scores_tmp[feature_name]))\n",
    "\n",
    "                score_max = scores_tmp.max()   # Store the score of the best performimg feature set\n",
    "\n",
    "                if(score_max > score_max_previous):   # If any of the features added in the last iteration has improved classification score, add the feature which improved score the most to the best feature set, store the best score and remove the feature from the features to be selected from\n",
    "                    score_max_previous = score_max\n",
    "                    best_features.append(scores_tmp.idxmax())\n",
    "                    remaining_features.remove(scores_tmp.idxmax())\n",
    "\n",
    "                    print_log('{}Added feature: \\033[1m{}\\033[0m Score: \\033[1m{}\\033[0m{}'.format(format_char, scores_tmp.idxmax(), score_max, format_char))\n",
    "                else:   # If none of the features added in the last iteration improved classification score, stop the selection\n",
    "                    print_log('\\nAdding any of the remaining features did not improve score, stopping.')\n",
    "                    break\n",
    "\n",
    "            # Add the classifier name to the optimised feature sets dictionary if not there already\n",
    "            if not classifier_name in optimised_feature_sets:\n",
    "                optimised_feature_sets[classifier_name] = {}\n",
    "                \n",
    "            # Name the best performing feature set according to the selection and validation methods used and store it\n",
    "            name = 'opt_feature_set_FS_CV' if parameters['use_cross_validation'] else 'opt_feature_set_FS_VS'\n",
    "            optimised_feature_sets[classifier_name][name] = best_features\n",
    "\n",
    "            print_log('\\nSelecting features for classifier \\033[1m{}\\033[0m finished in ~\\033[1m{}\\033[0m.'.format(classifier_name, str(timedelta(seconds=(time.time() - t2))).split(\".\")[0]))\n",
    "            print_log('Best feature subset FS: \\033[1m{}\\033[0m'.format(best_features))\n",
    "            print_log('Score: \\033[1m{}\\033[0m'.format(score_max_previous))\n",
    "            print_log('-'*100)\n",
    "\n",
    "        print_log('\\nSelecting features using forward selection method finished in ~\\033[1m{}\\033[0m.'.format(str(timedelta(seconds=(time.time() - t1))).split(\".\")[0]))\n",
    "        print_log('*'*100)\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "    # BACKWARD ELIMINATION\n",
    "    \n",
    "    if parameters['use_backward_elimination']:   # If backward elimination is selected\n",
    "        print_log('Selecting features using backward elimination method:')\n",
    "        t1 = time.time()   # Store the start time of backward elimination\n",
    "\n",
    "        for classifier_name, classifier in tqdm(classifiers.items(), desc='classifiers', leave=True):   # For each of the selected classifiers\n",
    "            print_log('\\nSelecting features for classifier \\033[1m{}\\033[0m:\\n'.format(classifier_name))\n",
    "            t2 = time.time()# Store the start time of backward elimination for the classifier\n",
    "\n",
    "            classifier = classifier(**default_params[classifier_name])   # Initialize the classifier\n",
    "            n_jobs = parameters['cross_validation_num_of_folds'] if classifier_name in ['LogisticRegression', 'DecisionTreeClassifier', 'SVC_linear', 'SVC_rbf'] else 1   # Set number of processes for cross-validation for classifiers which do not support multiprocessing\n",
    "            n_jobs = n_jobs if n_jobs <= num_cpus else num_cpus   # Limit number of processes to CPU cores\n",
    "            best_features = feature_names.copy()   # Stores the best selected features\n",
    "            X = features[best_features].values   # Get the values of the features\n",
    "            X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)   # Split the samples to train and test data (test data is not used)\n",
    "            \n",
    "            # Perform classification with all features to get the score to compare results with\n",
    "            if parameters['use_cross_validation']:   # If cross-validation is selected, transform all the training data and perform cross-validation                         \n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                scores_tmp[feature_name] = cross_val_score(classifier, X_train, y_train, cv=StratifiedKFold(n_splits=parameters['cross_validation_num_of_folds']), n_jobs=n_jobs).mean()\n",
    "            else:   # If validation set is selected, split the training data to training and validation sets, fit the classifier and perform classification on the validation set\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=parameters['validation_set_size'], random_state=42, stratify=y_train)\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_val = scaler.transform(X_val)\n",
    "                classifier.fit(X_train, y_train)\n",
    "                score_max_previous = classifier.score(X_val, y_val)\n",
    "                \n",
    "            print_log('Initial score: \\033[1m{}\\033[0m'.format(score_max_previous))\n",
    "\n",
    "            for i in range(len(feature_names)):   # For each feature name\n",
    "                scores_tmp = pd.Series(index=best_features, dtype='float64')   # Stores scores for all feature sets in each iteration\n",
    "\n",
    "                for feature_name in best_features:   # For each of the remaining features\n",
    "                    current_features = best_features.copy()   # Restore remaining features\n",
    "                    current_features.remove(feature_name)   # Remove the feature\n",
    "                    X = features[current_features].values   # Get the values of the features\n",
    "                    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)   # Split the samples to train and test data (test data is not used)\n",
    "                    \n",
    "                    # Perform classification with the currently selected features\n",
    "                    if parameters['use_cross_validation']:   # If cross-validation is selected, transform all the training data and perform cross-validation                           \n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        scores_tmp[feature_name] = cross_val_score(classifier, X_train, y_train, cv=StratifiedKFold(n_splits=parameters['cross_validation_num_of_folds']), n_jobs=n_jobs).mean()\n",
    "                    else:   # If validation set is selected, split the training data to training and validation sets, fit the classifier and perform classification on the validation set\n",
    "                        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=parameters['validation_set_size'], random_state=42, stratify=y_train)\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        X_val = scaler.transform(X_val)\n",
    "                        classifier.fit(X_train, y_train)\n",
    "                        scores_tmp[feature_name] = classifier.score(X_val, y_val)\n",
    "\n",
    "                    if parameters['verbose']:\n",
    "                        print_log('Score without feature \\033[1m{}\\033[0m: \\033[1m{}\\033[0m'.format(feature_name, scores_tmp[feature_name]))\n",
    "\n",
    "                score_max = scores_tmp.max()\n",
    "\n",
    "                if(score_max >= score_max_previous):   # If removing any of the features in the last iteration has improved classification score, remove the feature which improved score the most from the best feature set and store the best score\n",
    "                    score_max_previous = score_max\n",
    "                    best_features.remove(scores_tmp.idxmax())\n",
    "\n",
    "                    print_log('{}Removed feature: \\033[1m{}\\033[0m Score: \\033[1m{}\\033[0m{}'.format(format_char, scores_tmp.idxmax(), score_max, format_char))\n",
    "                else:   # If removing any of the features in the last iteration did not improve classification score, stop the selection\n",
    "                    print_log('\\nRemoving any of the remaining features did worsen score, stopping.')\n",
    "                    break\n",
    "\n",
    "            # Add the classifier name to the optimised feature sets dictionary if not there already\n",
    "            if not classifier_name in optimised_feature_sets:\n",
    "                optimised_feature_sets[classifier_name] = {}\n",
    "                \n",
    "            # Name the best performing feature set according to the selection and validation methods used and store it\n",
    "            name = 'opt_feature_set_BE_CV' if parameters['use_cross_validation'] else 'opt_feature_set_BE_VS'\n",
    "            optimised_feature_sets[classifier_name][name] = best_features\n",
    "\n",
    "            print_log('\\nSelecting features for classifier \\033[1m{}\\033[0m finished in ~\\033[1m{}\\033[0m.'.format(classifier_name, str(timedelta(seconds=(time.time() - t2))).split(\".\")[0]))\n",
    "            print_log('Best feature subset BE: \\033[1m{}\\033[0m'.format(best_features))\n",
    "            print_log('Score: \\033[1m{}\\033[0m'.format(score_max_previous))\n",
    "            print_log('-'*100)\n",
    "\n",
    "        print_log('\\nSelecting features using backward elimination method finished in ~\\033[1m{}\\033[0m.'.format(str(timedelta(seconds=(time.time() - t1))).split(\".\")[0]))\n",
    "        print_log('*'*100)\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "        \n",
    "    return optimised_feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka slouží k nastavení potřebných parametrů a výběru datových sad, způsobů selekce a klasifikátorů, pro které má být selekce provedena. <br>\n",
    "Pro jaké datové sady má být selekce provedena je možné zvolit v proměnné <code>datasets</code> odkomentováním/zakomentováním příslušných záznamů. <br>\n",
    "Při použití vlastní datové sady je nutné do proměnné <code>datasets</code> přidat stejný název datové sady, jako byl použit pro extrakci atributů v souboru <strong>feature_extraction.ipynb</strong> <br>\n",
    "Dále je nutné zvolit, jestli selekci provést na atributech extrahovaných pomocí knihovny Librosa či Essentia odkomentováním/zakomentováním příslušných záznamů v proměnné <code>feature_extraction_libraries</code>. <br>\n",
    "Pro které klasifikátory má být selekce provedena je možné zvolit v proměnné <code>classifiers</code> odkomentováním/zakomentováním příslušných záznamů. <br>\n",
    "Při použití jiného klasifikátorů je tento nutné přidat do proměnné <code>classifiers</code> ve formátu {zvolený_název_klasifikátoru}:{odkaz_na_objekt_klasifikátoru}. <br>\n",
    "Paremetry klasifikátorů je možné upravit v proměnné <code>default_params</code>, případně pro nový klasifikátor přidat záznam ve formátu {zvolený_název_klasifikátoru}:{slovník_parametrů}, kde zvolený název klasifikátoru musí odpovídat názvu v proměnné <code>classifiers</code>. <br>\n",
    "V proměnné <code>parameters</code> je možné nastavit průběh selekce. <br>\n",
    "<br>\n",
    "Popis parametrů: <br>\n",
    "<ul>\n",
    "    <li><code>use_forward_selection</code> Hodnota True znamená použití dopředné selekce</li>\n",
    "    <li><code>use_backward_elimination</code> Hodnota True znamená použití zpětné eliminace</li>\n",
    "    <li><code>use_cross_validation</code> Hodnota True znamená použití křížové validace, hodnota False pak validace na validační sadě</li>\n",
    "    <li><code>cross_validation_num_of_folds</code> Značí, kolikanásobná bude křížová validace, pokud je tato metoda zvolena</li>\n",
    "    <li><code>validation_set_size</code> Značí, jaká část trénovacích dat bude vyhrazena jako validační, pokud je tato metoda zvolena</li>\n",
    "    <li><code>verbose</code> Hodnota True znamená rozšířený výpis (vypsáno bude dosažené skóre pro každou vnitřní iteraci výběrů), hodnota False znamená výpis pouze finálně přidaných/odebraných atributů</li>\n",
    "</ul>\n",
    "\n",
    "Při nastavování parametrů je třeba brát v potaz, že selkece je velmi výpočetně náročný proces. Například při použití zpětné eliminace s pětinásobnou křížovou validací pro jeden klasifikátor na sadě 80 atributů extrahovaných pomocí knihovny Essentia bude při odstranění pouhých pěti atributů klasifikátor natrénován celekm 1+((80+79+78+77+76)*5) = 1951 krát! <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    # List of datasets whose features should be used, unwanted can be commented out   \n",
    "    datasets = [\n",
    "#         'EBD',\n",
    "#         'FMA',\n",
    "        'GTZAN'\n",
    "    ]\n",
    "    \n",
    "    # List of feature extraction libraries whose features should be used, unwanted can be commented out   \n",
    "    feature_extraction_libraries = [\n",
    "        'librosa',\n",
    "#         'essentia'\n",
    "    ]\n",
    "    \n",
    "    # Dictionary of classifier names and objects for which optimal feature subset should be found, unwanted can be commented out   \n",
    "    classifiers = {\n",
    "        # sklearn classifiers\n",
    "#         'LogisticRegression':LogisticRegression,\n",
    "        'KNeighborsClassifier':KNeighborsClassifier,\n",
    "#         'MLPClassifier':MLPClassifier,\n",
    "#         'DecisionTreeClassifier':DecisionTreeClassifier,\n",
    "#         'SVC_linear':SVC,\n",
    "#         'SVC_rbf':SVC,\n",
    "        \n",
    "        # sklearn ensemble classifiers\n",
    "#         'RandomForestClassifier':RandomForestClassifier,\n",
    "        \n",
    "        # other classifiers\n",
    "#         'XGBClassifier':XGBClassifier,\n",
    "    }\n",
    "    \n",
    "    # Default classifiers parameters to use \n",
    "    default_params = {\n",
    "        'LogisticRegression': {'max_iter':10000, 'class_weight':'balanced'},\n",
    "        'KNeighborsClassifier': {'n_jobs':-1, 'algorithm':'brute'},\n",
    "        'MLPClassifier': {'max_iter':10000, 'random_state':42},\n",
    "        'DecisionTreeClassifier': {'class_weight':'balanced', 'random_state':42},\n",
    "        'SVC_linear': {'kernel':'linear', 'class_weight':'balanced'},\n",
    "        'SVC_rbf': {'kernel':'rbf', 'class_weight':'balanced'},\n",
    "        'RandomForestClassifier': {'n_jobs':-1, 'class_weight':'balanced', 'random_state':42},\n",
    "        'XGBClassifier': {'tree_method':'gpu_hist', 'n_jobs':1, 'random_state':42},\n",
    "    }\n",
    "    \n",
    "    # Parameters for the feature selection\n",
    "    parameters = {\n",
    "        'use_forward_selection' : True,   # Set to true to use forward selection method\n",
    "        'use_backward_elimination' : True,   # Set to true to use backward elimination method\n",
    "        'use_cross_validation': False,   # Set to true to use cross-validation on the entire training data, else a portion of the train data will be reserved as validation set and cross-validation will not be used\n",
    "        'cross_validation_num_of_folds' : 5,   # Determines the number of cross-validation folds, ignored if 'use_cross_validation' parameter is set to False\n",
    "        'validation_set_size': 0.2,   # Determines the size of the portion of training data, which will be reserved as validation set, ignored if 'use_cross_validation' parameter is set to True\n",
    "        'verbose' : False   # Set to True to print out each of the tried out combinations of features and its scores\n",
    "    }\n",
    "    \n",
    "    # Load optimised feature sets file if available\n",
    "    try:\n",
    "        with open('../metadata/misc/optimised_feature_sets.json') as f:\n",
    "            optimised_feature_sets = json.load(f)            \n",
    "    except FileNotFoundError:    \n",
    "        optimised_feature_sets = {}\n",
    "        \n",
    "    if parameters['use_cross_validation']:\n",
    "        print_log('Using cross-validation.')\n",
    "    else:\n",
    "        print_log('Using validation set.')\n",
    "    \n",
    "    t_start = time.time()   # Store the start time of the feature selection \n",
    "    \n",
    "    for dataset in datasets:   # For each of the selected datasets\n",
    "        \n",
    "        # Add dataset name to optimised feature sets dictionary if not there already\n",
    "        if not dataset in optimised_feature_sets:\n",
    "            optimised_feature_sets[dataset] = {}\n",
    "        \n",
    "        for library in feature_extraction_libraries:   # For each of the selected extraction libraries features\n",
    "            \n",
    "            # Add extraction library name to optimised feature sets dictionary if not there already\n",
    "            if not library in optimised_feature_sets[dataset]:\n",
    "                optimised_feature_sets[dataset][library] = {}\n",
    "            \n",
    "            print_log('#'*100)\n",
    "            print_log('Selecting features from \\033[1m{}\\033[0m dataset features extracted with \\033[1m{}\\033[0m library:\\n'.format(dataset, library))\n",
    "            t = time.time()   # Store the start time of feature selection for selected dataset and extraction library features\n",
    "            get_feature_sets(optimised_feature_sets[dataset][library], dataset, library, classifiers, default_params, parameters)\n",
    "            print_log(\"\\nSelecting features from \\033[1m{}\\033[0m dataset features extracted with \\033[1m{}\\033[0m library finished in ~\\033[1m{}\\033[0m.\".format(dataset, library, str(timedelta(seconds=(time.time() - t))).split(\".\")[0]))\n",
    "            print_log('#'*100)\n",
    "\n",
    "    # Save optimised feature sets to .json file\n",
    "    with open('../metadata/misc/optimised_feature_sets.json', 'w') as json_file:\n",
    "        json.dump(optimised_feature_sets, json_file)\n",
    "        \n",
    "    print_log(\"Selecting features from all selected datasets and all selected features finished in ~\\033[1m{}\\033[0m.\".format(str(timedelta(seconds=(time.time() - t_start))).split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
