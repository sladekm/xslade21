{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Matyáš Sládek <br>\n",
    "Rok: 2020 <br>\n",
    "\n",
    "Tento soubor slouží k trénování a klasifikaci datových sad s možností použití všech sad atributů a parametrů. Dále je možné natrénované klasifikátory uložit a načíst a také zobrazit či uložit matice predikcí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka importuje potřebné knihovny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka obsahuje funkci pro načtení a zpracování dat, trénování, načítání a ukládání klasifikačních algoritmů a klasifikaci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_classification(dataset, library, classifiers, default_params, parameters):\n",
    "    '''\n",
    "    Function loads and processes required data and performs training and classification.\n",
    "    Trained classifiers can be saved and loaded and confusion matrices shown or saved.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    optimised_feature_sets: Dictionary containing optimised feature sets\n",
    "    dataset:                Name of a dataset to indicate which extracted features to load (dataset from which the features were extracted)\n",
    "    library:                Name of a library to indicate which extracted features to load (library with which the features were extracted)\n",
    "    classifiers:            Dictionary containing classifiers for which the optimal feature subset should be found\n",
    "    default_params:         Dictionary containing parameters for classifiers\n",
    "    parameters:             Parameters about which selection to use etc.\n",
    "    '''\n",
    "    \n",
    "    def classify():\n",
    "        '''\n",
    "        Inner function that performs training and classification.\n",
    "        Trained classifiers can be saved and loaded and confusion matrices shown or saved.\n",
    "        This function has access to parameters in scope of the outer function.\n",
    "        '''\n",
    "        \n",
    "        clfs = {}   # Stores classifier objects\n",
    "\n",
    "        if parameters['use_default_hyper_parameters']:   # If selected, create unique name for classifier with parameters default and initialize classifier with default parameters\n",
    "                                                         # or set object to None if pretrained classifier should be loaded\n",
    "            try:\n",
    "                clfs[classifier_name + '_default'] = None if parameters['load_pretrained_classifiers'] else classifier(**default_params[classifier_name])\n",
    "            except KeyError:\n",
    "                print('Default parameters for classifier {} not found!'.format(classifier_name), file=sys.stderr)\n",
    "\n",
    "        if parameters['use_optimised_hyper_parameters_CV']:   # If selected, create unique name for classifier with parameters optimised using cross-validation and initialize classifier with default parameters\n",
    "                                                              # or set object to None if pretrained classifier should be loaded\n",
    "            try:\n",
    "                clfs[classifier_name + '_optimised_CV'] = None if parameters['load_pretrained_classifiers'] else classifier(**default_params[classifier_name], **optimised_parameters[dataset][library][feature_set_name][classifier_name]['CV'])\n",
    "            except KeyError:\n",
    "                print('{} {} {} {} {} Optimised hyper parameters not found!'.format(dataset, library, feature_set_name, classifier_name, 'CV'), file=sys.stderr)\n",
    "\n",
    "        if parameters['use_optimised_hyper_parameters_VS']:   # If selected, create unique name for classifier with parameters optimised using validation set and initialize classifier with default parameters  \n",
    "                                                              # or set object to None if pretrained classifier should be loaded\n",
    "            try:\n",
    "                clfs[classifier_name + '_optimised_VS'] = None if parameters['load_pretrained_classifiers'] else classifier(**default_params[classifier_name], **optimised_parameters[dataset][library][feature_set_name][classifier_name]['VS'])\n",
    "            except KeyError:\n",
    "                print('{} {} {} {} {} Optimised hyper parameters not found!'.format(dataset, library, feature_set_name, classifier_name, 'VS'), file=sys.stderr)\n",
    "                \n",
    "        for clf_name, clf in clfs.items():   # For each of the selected classifiers\n",
    "            t = time.time()   # Store start time of the optimisation process\n",
    "            \n",
    "            if parameters['load_pretrained_classifiers']:   # Load pretrained classifiers if selected\n",
    "                try:\n",
    "                    clf = joblib.load('../metadata/trained_classifiers/{}_{}_{}_{}.joblib.dat'.format(dataset, library, feature_set_name, clf_name))\n",
    "                except Exception as e:\n",
    "                    print('Failed to read file: \"../metadata/trained_classifiers/{}_{}_{}_{}.joblib.dat\"'.format(dataset, library, feature_set_name, clf_name), file=sys.stderr)\n",
    "                    print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "                    continue\n",
    "            else:\n",
    "                clf.fit(X_train, y_train)   # Train the classifier\n",
    "                \n",
    "                if parameters['save_trained_classifiers']:   # Save the trained classifier if selected\n",
    "                    \n",
    "                    if not parameters['use_test_set']:\n",
    "                        print('Saving classifiers should be done when test set is selected to use all training data.', file=sys.stderr)\n",
    "                    \n",
    "                    joblib.dump(clf, '../metadata/trained_classifiers/{}_{}_{}_{}.joblib.dat'.format(dataset, library, feature_set_name, clf_name)) \n",
    "            \n",
    "            # Perform classification either on test set or validation set using F1 or accuracy scoring\n",
    "            if parameters['use_test_set']:               \n",
    "                if parameters['use_f1_score']:\n",
    "                    score = f1_score(y_test, clf.predict(X_test), average='macro')\n",
    "                else:\n",
    "                    score = clf.score(X_test, y_test)\n",
    "            else:\n",
    "                if parameters['use_f1_score']:\n",
    "                    score = f1_score(y_val, clf.predict(X_val), average='macro')\n",
    "                else:\n",
    "                    score = clf.score(X_val, y_val)\n",
    "                \n",
    "            scores.loc[feature_set_name, clf_name] = score   # Save classification score to dataframe\n",
    "            runtimes.loc[feature_set_name, clf_name] = str(timedelta(seconds=(time.time() - t))).split(\".\")[0]   # Save training and classification time to dataframe\n",
    "\n",
    "            # Show or save confusion matrix if selected\n",
    "            if parameters['save_confusion_matrices'] or parameters['show_confusion_matrices']:\n",
    "                cm = confusion_matrix(y_test, clf.predict(X_test), normalize='true')   # Create normalized confusion matrix\n",
    "                df_cm = pd.DataFrame(cm, index = encoder.classes_, columns = encoder.classes_)   # Create dataframe from confusion matrix with correct classes on axes\n",
    "                fig, ax = plt.subplots(figsize = (10,7))   # Create matplotlib figure\n",
    "                ax.set_title('{} {} {} {} confusion matrix'.format(dataset, library, feature_set_name, clf_name))   # Set title to figure\n",
    "                fmt = lambda x,pos: '{:.0%}'.format(x)   # Format colorbar values to percentage\n",
    "                ax = sns.heatmap(df_cm, cbar_kws={'format': FuncFormatter(fmt)}, annot=True, fmt=\".2%\")   # Create heatmap from confusion matrix with annotated values (percentage format)\n",
    "                ax.set(xlabel=\"Predicted label\", ylabel = \"True label\")   # Name axes\n",
    "                plt.tight_layout()   # Correct layout to fit figure\n",
    "\n",
    "                if parameters['save_confusion_matrices']:   # Save confusion matrix to PDF if selected\n",
    "                    \n",
    "                    # Create folder for storing confusion matrices\n",
    "                    if not os.path.exists('../metadata/confusion_matrices'):\n",
    "                        try:\n",
    "                            os.mkdir('../metadata/confusion_matrices')\n",
    "                        except Exception as e:\n",
    "                            print_log('{}: {}'.format('os.mkdir(../metadata/confusion_matrices)', repr(e)), file=sys.stderr)\n",
    "                    \n",
    "                    plt.savefig(\"../metadata/confusion_matrices/{}_{}_{}_{}_{}.pdf\".format(dataset, library, feature_set_name, clf_name, 'TS' if parameters['use_test_set'] else 'VS'))\n",
    "\n",
    "                if not parameters['show_confusion_matrices']:   # Close plot to not be shown automatically if selected\n",
    "                    plt.close()\n",
    "                    \n",
    "        return\n",
    "        \n",
    "        \n",
    "    # If only test set is selected to be used            \n",
    "    if parameters['load_test_set']:\n",
    "        \n",
    "        # Automatically select correct parameter values if not selected already\n",
    "        parameters['use_test_set'] = True\n",
    "        parameters['load_pretrained_classifiers'] = True\n",
    "        parameters['save_trained_classifiers'] = False\n",
    "        \n",
    "        feature_set_name = 'all'   # Set correct feature set name for the test sets (only test sets with all features can be used due to scaling)\n",
    "        scores = pd.DataFrame(index=[feature_set_name])   # Init dataframe to store scores\n",
    "        runtimes = pd.DataFrame(index=[feature_set_name])   # Init dataframe to store prediction times\n",
    "        X_test = pd.read_csv(\"../metadata/test_data/{}_X_test.csv\".format(dataset), header=None)   # Load test data (already scaled)\n",
    "        X_test = X_test.values   # Transform dataframe to ndarray\n",
    "\n",
    "        genres = pd.read_csv(\"../metadata/test_data/{}_genres.csv\".format(dataset), index_col=0, header=0)   # Load test data target genres\n",
    "        encoder = LabelEncoder()   # Init label encoder\n",
    "        y_test = encoder.fit_transform(np.ravel(genres))   # Encode genres\n",
    "        \n",
    "        # For each selected classifier perform classification\n",
    "        for classifier_name, _ in classifiers.items():\n",
    "            classify()\n",
    "            \n",
    "        return(scores, runtimes)\n",
    "    \n",
    "    optimised_feature_sets_names = []   # Stores the names of optimised feature sets to be used for classification\n",
    "    X_val = None   # Stores the features of validation set if used\n",
    "    y_val = None   # Stores the genres of validation set if used\n",
    "    \n",
    "    # If any of the optimised feature sets is selected\n",
    "    if parameters['use_opt_feature_set_FS_CV'] or parameters['use_opt_feature_set_FS_VS'] or parameters['use_opt_feature_set_BE_CV'] or parameters['use_opt_feature_set_BE_VS']:\n",
    "        \n",
    "        # Load optimised feature sets if available\n",
    "        try:\n",
    "            with open('../metadata/misc/optimised_feature_sets.json') as f:\n",
    "                optimised_feature_sets = json.load(f)   \n",
    "        except Exception as e:\n",
    "            print('Failed to read file: \"../metadata/misc/optimised_feature_sets.json\"!', file=sys.stderr)\n",
    "            print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "            return -1\n",
    "        \n",
    "        # If feature set optimised with forward selection and cross-validation is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_FS_CV']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_FS_CV')\n",
    "            \n",
    "        # If feature set optimised with forward selection and validation set is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_FS_VS']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_FS_VS')\n",
    "            \n",
    "        # If feature set optimised with backward elimination and cross-validation is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_BE_CV']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_BE_CV')\n",
    "            \n",
    "        # If feature set optimised with backward elimination and validation set is selected, add it to the dictionary of feature sets to be optimised\n",
    "        if parameters['use_opt_feature_set_BE_VS']:\n",
    "            optimised_feature_sets_names.append('opt_feature_set_BE_VS')\n",
    "\n",
    "    # Load optimised hyper parameters\n",
    "    if parameters['use_optimised_hyper_parameters_CV'] or parameters['use_optimised_hyper_parameters_VS']:\n",
    "        try:\n",
    "            with open('../metadata/misc/optimised_hyper_parameters.json') as f:\n",
    "                optimised_parameters = json.load(f) \n",
    "        except Exception as e:\n",
    "            print('Failed to read file: \"../metadata/misc/optimised_hyper_parameters.json\"!', file=sys.stderr)\n",
    "            print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "            return -1\n",
    "    \n",
    "    # Load specified extracted features\n",
    "    try:                \n",
    "        features = pd.read_csv('../metadata/features/features_{}_{}.csv'.format(dataset, library), index_col=0, header=[0, 1, 2])        \n",
    "    except Exception as e:\n",
    "        print('Failed to read file: \"../metadata/features/features_{}_{}.csv\"!'.format(dataset, library), file=sys.stderr)\n",
    "        print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "        \n",
    "    # Perform One-hot encoding on categorical features\n",
    "    # This method is modified so that there is always full number of columns generated, even if not all cases of categorical features are present\n",
    "    # This is redundant when EBD, FMA or GTZAN datasets are used, but when a smaller dataset would be used, \n",
    "    # it would ensure properly working predicitions with trained classifiers on data from a different dataset\n",
    "    \n",
    "    # Perform One-hot encoding on categorical features\n",
    "    for column in features.select_dtypes(include='object'):   # For each categorical column\n",
    "        dummy_columns = pd.get_dummies(features[column])   # Encode the column values   \n",
    "        features = features.drop(columns=column)   # Drop the column from the dataframe\n",
    "\n",
    "        # Reindex columns to fixed length with all possible instances to avoid feature mismatch with trained classifiers\n",
    "        if (column[0] in ['chords_key', 'key_edma', 'key_krumhansl', 'key_temperley']) and (column[1] in ['none', 'key']):\n",
    "            dummy_columns = dummy_columns.reindex(['Ab', 'B', 'Bb', 'C', 'C#', 'D', 'E', 'Eb', 'F', 'F#', 'G'], axis=1, fill_value=0)\n",
    "        elif (column[0] in ['chords_scale', 'key_edma', 'key_krumhansl', 'key_temperley']) and (column[1] in ['none', 'scale']):\n",
    "            dummy_columns = dummy_columns.reindex(['minor'], axis=1, fill_value=0)\n",
    "\n",
    "        # Create correct multiindex for the encoded columns and append them to the features dataframe\n",
    "        dummy_columns.columns = pd.MultiIndex.from_product([[column[0]], [column[1]], ['{}'.format(c) for c in dummy_columns.columns]], names=features.columns.names)\n",
    "        features = pd.concat([features, dummy_columns], axis=1).sort_index(axis=1)\n",
    "                \n",
    "    feature_names = list(features.columns.levels[0])   # Get names of all features    \n",
    "    feature_sets = {}\n",
    "    \n",
    "    # If all features is selected, add all feature names to the dictionary of feature sets to be used\n",
    "    if parameters['use_all_features']:\n",
    "        feature_sets['all'] = feature_names\n",
    "        \n",
    "    # Can be uncommented to use each feature separately to evaluate them etc.\n",
    "#     for name in feature_names:\n",
    "#         feature_sets[name] = name\n",
    "            \n",
    "    scores = pd.DataFrame(index=list(feature_sets.keys()) + optimised_feature_sets_names)   # Init dataframe to store classification scores\n",
    "    runtimes = pd.DataFrame(index=list(feature_sets.keys()) + optimised_feature_sets_names)   # Init dataframe to store training and/or prediction runtimes\n",
    "        \n",
    "    # Load track-genre list\n",
    "    try:                \n",
    "        genres = pd.read_csv(\"../metadata/track_genre_lists/{}_track_genre_list.csv\".format(dataset), index_col=0, header=0)        \n",
    "    except Exception as e:\n",
    "        print('Failed to read file: \"../metadata/track_genre_lists/{}_track_genre_list.csv\"!'.format(dataset), file=sys.stderr)\n",
    "        print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "    \n",
    "    genres = genres.loc[features.index]   # Remove unwanted data from track-genre list (data about tracks removed from features because of corruption or other reasons)\n",
    "\n",
    "    # Encode genre labels\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(np.ravel(genres))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "        \n",
    "    # For each of the non-optimised feature sets\n",
    "    for feature_set_name, feature_set in feature_sets.items():\n",
    "        X = features[feature_set].values   # Extract selected feature set values to ndarray\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)   # Split genres and features to train and test sets (stratified)\n",
    "        \n",
    "        if parameters['use_test_set']:   # If test set is selected, scale features according to values in whole train set\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        else:   # Else split train data to new train set and validation set and scale features according to values in the new train set\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=parameters['validation_set_size'], random_state=42, stratify=y_train)\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_val = scaler.transform(X_val)\n",
    "\n",
    "        # Perform classification for each selected classifier\n",
    "        for classifier_name, classifier in classifiers.items():                            \n",
    "            classify()\n",
    "            \n",
    "    # This loop is included because with optimised feature sets data have to be splitted for each classifier separately,\n",
    "    # therefore having two separate loops for optimised and non-optimised feature sets is more efficient\n",
    "    for feature_set_name in optimised_feature_sets_names:   # For each selected optimised feature set        \n",
    "        for classifier_name, classifier in classifiers.items():   # For each selected classifier\n",
    "            \n",
    "            # Load optimised feature sets\n",
    "            try:\n",
    "                feature_set = optimised_feature_sets[dataset][library][classifier_name][feature_set_name]\n",
    "            except KeyError:\n",
    "                print('Optimised feature set {} for classifier {} not found!'.format(feature_set_name, classifier_name), file=sys.stderr)\n",
    "                continue\n",
    "            \n",
    "            X = features[feature_set].values   # Extract selected feature set values to ndarray\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)    # Split genres and features to train and test sets (stratified)\n",
    "\n",
    "            if parameters['use_test_set']:   # If test set is selected, scale features according to values in whole train set\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "            else:      # Else split train data to new train set and validation set and scale features according to values in the new train set\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=parameters['validation_set_size'], random_state=42, stratify=y_train)\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_val = scaler.transform(X_val)\n",
    "            \n",
    "            classify()   # Perform classification\n",
    "\n",
    "    return(scores, runtimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka slouží k nastavení potřebných parametrů a výběru datových sad, sad atributů, klasifikátorů a jejich parametrů, s pomocí kterých má být klasifikace provedena. <br>\n",
    "Pro jaké datové sady má být klasifikace provedena je možné zvolit v proměnné <code>datasets</code> odkomentováním/zakomentováním příslušných záznamů. <br>\n",
    "Při použití vlastní datové sady je nutné do proměnné <code>datasets</code> přidat stejný název datové sady, jako byl použit pro extrakci atributů v souboru <strong>feature_extraction.ipynb</strong> <br>\n",
    "Dále je nutné zvolit, jestli klasifikaci provést na atributech extrahovaných pomocí knihovny Librosa či Essentia odkomentováním/zakomentováním příslušných záznamů v proměnné <code>feature_extraction_libraries</code>. <br>\n",
    "Které klasifikátory mají být pro klasifikaci využity je možné zvolit v proměnné <code>classifiers</code> odkomentováním/zakomentováním příslušných záznamů. <br>\n",
    "Při použití jiného klasifikátorů je tento nutné přidat do proměnné <code>classifiers</code> ve formátu {zvolený_název_klasifikátoru}:{odkaz_na_objekt_klasifikátoru}. <br>\n",
    "Paremetry klasifikátorů je možné upravit v proměnné <code>default_params</code>, případně pro nový klasifikátor přidat záznam ve formátu {zvolený_název_klasifikátoru}:{slovník_parametrů}, kde zvolený název klasifikátoru musí odpovídat názvu v proměnné <code>classifiers</code>. <br>\n",
    "V proměnné <code>parameters</code> je možné nastavit parametry klasifikace. <br>\n",
    "<br>\n",
    "Popis parametrů: <br>\n",
    "<ul>\n",
    "    <li><code>use_f1_score</code> Hodnota true značí použití F1 skóre, hodnota False klasické skóre (počet správných / počet celkem)</li>\n",
    "    <li><code>load_test_set</code> Hodnota True značí použití uložené testovací sady, tato možnost automaticky zaručí použití celé sady atributů a načtení natrénovaných klasifikátorů nehledě na hodnoty parametrů</li>\n",
    "    <li><code>use_test_set</code> Hodnota True snačí použití testovací sady, hodnota False použití validační sady</li>\n",
    "    <li><code>validation_set_size</code> Značí, jaká část trénovacích dat bude vyhrazena jako validační, pokud je tato metoda zvolena</li>\n",
    "    <li><code>use_all_features</code> Hodnota True znamená použití celé sady atributů</li>\n",
    "    <li><code>use_opt_feature_set_FS_CV</code> Hodnota True znamená použití sady atributů vybrané pomocí metody dopředné selekce a křížové validace</li>\n",
    "    <li><code>use_opt_feature_set_FS_VS</code> Hodnota True znamená použití sady atributů vybrané pomocí metody dopředné selekce a validace na validační saďe</li>\n",
    "    <li><code>use_opt_feature_set_BE_CV</code> Hodnota True znamená použití sady atributů vybrané pomocí metody zpětné eliminace a křížové validace</li>\n",
    "    <li><code>use_opt_feature_set_BE_VS</code> Hodnota True znamená použití sady atributů vybrané pomocí metody zpětné eliminace a validace na validační saďe</li>\n",
    "    <li><code>use_default_hyper_parameters</code> Hodnota True značí použití neoptimalizovaných parametrů</li>\n",
    "    <li><code>use_optimised_hyper_parameters_CV</code> Hodnota True značí použití optimalizovaných parametrů pomocí křížové validace</li>\n",
    "    <li><code>use_optimised_hyper_parameters_VS</code> Hodnota True značí použití optimalizovaných parametrů pomocí validační sady</li>\n",
    "    <li><code>save_trained_classifiers</code> Hodnota True značí uložení vybraných natrénovaných klasifikačních algoritmů na vybraných sadách atributů</li>\n",
    "    <li><code>load_pretrained_classifiers</code> Hodnota True značí použití uložených natrénovaných klasifikátorů</li>\n",
    "    <li><code>save_confusion_matrices</code> Hodnota true značí uložení matice predikcí do PDF pro každý vybraný klasifikátor na každé ze sad atributů</li>\n",
    "    <li><code>show_confusion_matrices</code> Hodnota tru značí zobrazení matice predikcí pro každý vybraný klasifikátor na každé ze sad atributů</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test set with F1 scoring.\n",
      "FMA dataset classification using features extracted with essentia library finished in 0:00:09.\n",
      "FMA essentia scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_1a53283a_d140_11ea_9f31_871ef7308c00row0_col0 {\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_1a53283a_d140_11ea_9f31_871ef7308c00\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >XGBClassifier_optimised_VS</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1a53283a_d140_11ea_9f31_871ef7308c00level0_row0\" class=\"row_heading level0 row0\" >all</th>\n",
       "                        <td id=\"T_1a53283a_d140_11ea_9f31_871ef7308c00row0_col0\" class=\"data row0 col0\" >64.21%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f23229fee50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMA essentia runtimes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XGBClassifier_optimised_VS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    XGBClassifier_optimised_VS\n",
       "all                    0:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets classification finished in 0:00:09.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "       \n",
    "    def highlight_max(s):\n",
    "        '''\n",
    "        Hilight the maximum value in selected axis of a dataframe.\n",
    "        '''\n",
    "        is_max = s == s.max()\n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "        \n",
    "    # List of datasets whose features should be used, unwanted can be commented out   \n",
    "    datasets = [\n",
    "#         'EBD',\n",
    "        'FMA',\n",
    "#         'GTZAN'\n",
    "    ]\n",
    "    \n",
    "    # List of feature extraction libraries whose features should be used, unwanted can be commented out   \n",
    "    feature_extraction_libraries = [\n",
    "#         'librosa',\n",
    "        'essentia'\n",
    "    ]\n",
    "    \n",
    "    # Dictionary of classifier names and objects to use for classification, unwanted can be commented out   \n",
    "    classifiers = {\n",
    "        # sklearn classifiers\n",
    "#         'LogisticRegression':LogisticRegression,\n",
    "#         'KNeighborsClassifier':KNeighborsClassifier,\n",
    "#         'MLPClassifier':MLPClassifier,\n",
    "#         'DecisionTreeClassifier':DecisionTreeClassifier,\n",
    "#         'SVC_linear':SVC,\n",
    "#         'SVC_rbf':SVC,\n",
    "        \n",
    "        # sklearn ensemble classifiers\n",
    "#         'RandomForestClassifier':RandomForestClassifier,\n",
    "        \n",
    "        # other classifiers\n",
    "        'XGBClassifier':XGBClassifier,\n",
    "    }\n",
    "    \n",
    "    # Default classifiers parameters to use \n",
    "    default_params = {\n",
    "        'LogisticRegression': {'max_iter':10000, 'class_weight':'balanced'},\n",
    "        'KNeighborsClassifier': {'n_jobs':-1, 'algorithm':'brute'},\n",
    "        'MLPClassifier': {'max_iter':10000, 'random_state':42},\n",
    "        'DecisionTreeClassifier': {'class_weight':'balanced', 'random_state':42},\n",
    "        'SVC_linear': {'kernel':'linear', 'class_weight':'balanced'},\n",
    "        'SVC_rbf': {'kernel':'rbf', 'class_weight':'balanced'},\n",
    "        'RandomForestClassifier': {'n_jobs':-1, 'class_weight':'balanced', 'random_state':42},\n",
    "        'XGBClassifier': {'tree_method':'gpu_hist', 'n_jobs':1, 'random_state':42},\n",
    "    }\n",
    "    \n",
    "    parameters = {\n",
    "        'use_f1_score': True,   # Set to true to use F1 men scoring, else use accuracy scoring\n",
    "        'load_test_set': False,   # Loads test data, automatically select test set with all features to be classified with pretrained classifiers\n",
    "        'use_test_set': True,   # Set to true to use test set, else a portion of the train data will be reserved as validation set\n",
    "        'validation_set_size': 0.2,   # Determines the size of the portion of training data, which will be reserved as validation set, ignored if 'use_test_set' parameter is set to True\n",
    "        'use_all_features': True,   # Use all available features extracted with selected library\n",
    "        'use_opt_feature_set_FS_CV': False,   # Set this to true to use feature set optimised with forward selection and cross-validation\n",
    "        'use_opt_feature_set_FS_VS': False,   # Set this to true to use feature set optimised with backward elimination and validation set\n",
    "        'use_opt_feature_set_BE_CV': False,   # Set this to true to use feature set optimised with forward selection and cross-validation\n",
    "        'use_opt_feature_set_BE_VS': False,   # Set this to true to use feature set optimised with backward elimination and validation set\n",
    "        'use_default_hyper_parameters': False,   # Set this to True to use default hyper parameters\n",
    "        'use_optimised_hyper_parameters_CV': False,   # Set this to True to use hyper parameters optimised using cross-validation\n",
    "        'use_optimised_hyper_parameters_VS': True,   # Set this to True to use hyper parameters optimised using validation set\n",
    "        'save_trained_classifiers': False,   # Save trained classifiers\n",
    "        'load_pretrained_classifiers': True,   # Load pretrained classifiers\n",
    "        'save_confusion_matrices': False,   # Save confusion matrix for each classifier\n",
    "        'show_confusion_matrices': False,   # Display confusion matrix for each classifier\n",
    "    }\n",
    "\n",
    "    if parameters['use_test_set']:\n",
    "        print('Using test set with {} scoring.'.format('F1' if parameters['use_f1_score'] else 'accuracy'))\n",
    "    else:\n",
    "        print('Using validation set. with {} scoring.'.format('F1' if parameters['use_f1_score'] else 'accuracy'))\n",
    "    \n",
    "    t_start = time.time()   # Store the start time of classification\n",
    "    \n",
    "    for dataset in datasets:   # For each of the selected datasets\n",
    "        \n",
    "        for library in feature_extraction_libraries:   # For each of the selected extraction libraries features\n",
    "            \n",
    "            t = time.time()   # Store the start time of classification for selected dataset and extraction library features\n",
    "            scores, runtimes = perform_classification(dataset, library, classifiers, default_params, parameters)\n",
    "            print(\"{} dataset classification using features extracted with {} library finished in {}.\".format(dataset, library, str(timedelta(seconds=(time.time() - t))).split(\".\")[0]))\n",
    "\n",
    "            # Create folder for storing confusion matrices\n",
    "            if not (os.path.exists('../metadata/scores') and os.path.exists('../metadata/runtimes')):\n",
    "                try:\n",
    "                    os.mkdir('../metadata/scores')\n",
    "                except Exception as e:\n",
    "                    print_log('{}: {}'.format('os.mkdir(../metadata/scores)', repr(e)), file=sys.stderr)\n",
    "                    \n",
    "                try:\n",
    "                    os.mkdir('../metadata/runtimes')\n",
    "                except Exception as e:\n",
    "                    print_log('{}: {}'.format('os.mkdir(../metadata/runtimes)', repr(e)), file=sys.stderr)\n",
    "            \n",
    "            scores.to_csv(\"../metadata/scores/scores_{}_{}_{}_{}.csv\".format(dataset, library, 'test_set' if parameters['use_test_set'] else 'validation_set', 'F1_score' if parameters['use_f1_score'] else 'accuracy_score'))   # Save the classification scores to a file\n",
    "            runtimes.to_csv(\"../metadata/runtimes/runtimes{}_{}_{}_{}.csv\".format('_predictions' if parameters['load_pretrained_classifiers'] else '', dataset, library, 'test_set' if parameters['use_test_set'] else 'validation_set'))   # Save the classification runtimes to a file\n",
    "\n",
    "            print(\"{} {} scores:\".format(dataset, library))\n",
    "            scores = scores.style.apply(highlight_max, axis=1)   # Apply higlight max styling to scores dataframe\n",
    "            ipd.display(scores.format(\"{:.2%}\"))   # Display the scores dataframe\n",
    "            print(\"{} {} runtimes:\".format(dataset, library))\n",
    "            ipd.display(runtimes)   # Display the runtimes dataframe\n",
    "    \n",
    "    print(\"All datasets classification finished in {}.\".format(str(timedelta(seconds=(time.time() - t_start))).split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
