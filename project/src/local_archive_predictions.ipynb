{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Matyáš Sládek <br>\n",
    "Rok: 2020 <br>\n",
    "\n",
    "Tento soubor slouží k predicki žánrů skladeb lokálního archivu a jejich anotaci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka importuje potřebné knihovny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mutagen.easyid3 import EasyID3\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka obsahuje funkci pro predicki žánrů skladeb z lokálního archivu, umožňuje také modifikovat ID3 tagy souborů skladeb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_local(classifier, parameters):\n",
    "    '''\n",
    "    This function is used to load and process specified data and predict track genres using selected trained classifier\n",
    "    Track files can be annotated with ID3 tag\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    classifier: Full name of the trained classifier file\n",
    "    parameters: local_archive name and whether to annotate files\n",
    "    '''\n",
    "    \n",
    "    # Initialise variables for storing classifier information\n",
    "    dataset = ''\n",
    "    library = ''\n",
    "    feature_set_name = ''\n",
    "    clf_name = ''\n",
    "\n",
    "    info = classifier.split('_')   # Split classifier name \n",
    "        \n",
    "    dataset = info[0]   # Set dataset name\n",
    "    library = info[1]   # Set feature exraction library name\n",
    "    \n",
    "    # Extract other info from classifier name (feature set name and classifier name)\n",
    "    if len(info) == 5:\n",
    "        feature_set_name = info[2]\n",
    "        clf_name = info[3] + '_' + info[4]\n",
    "    elif len(info) == 6:\n",
    "        feature_set_name = info[2]\n",
    "        clf_name = info[3] + '_' + info[4] + '_' + info[5]\n",
    "    elif len(info) == 7:\n",
    "        feature_set_name = info[2]\n",
    "        clf_name = info[3] + '_' + info[4] + '_' + info[5] + '_' + info[6]\n",
    "    elif len(info) == 9:\n",
    "        feature_set_name = info[2] + '_' + info[3] + '_' + info[4] + '_' + info[5] + '_' + info[6]\n",
    "        clf_name = info[7] + '_' + info[8]\n",
    "    elif len(info) == 10:\n",
    "        feature_set_name = info[2] + '_' + info[3] + '_' + info[4] + '_' + info[5] + '_' + info[6]\n",
    "        clf_name = info[7] + '_' + info[8] + '_' + info[9]\n",
    "    elif len(info) == 11:\n",
    "        feature_set_name = info[2] + '_' + info[3] + '_' + info[4] + '_' + info[5] + '_' + info[6]\n",
    "        clf_name = info[7] + '_' + info[8] + '_' + info[9] + '_' + info[10]\n",
    "    else:\n",
    "        print('Cannot process classifier name.', file=sys.stderr)\n",
    "    \n",
    "    # Load specified extracted features\n",
    "    try:                \n",
    "        features = pd.read_csv('../metadata/features/features_{}_{}.csv'.format(parameters['local_archive_name'], library), index_col=0, header=[0, 1, 2])        \n",
    "    except Exception as e:\n",
    "        print('Failed to read file: \"../metadata/features/features_{}_{}.csv\"!'.format(parameters['local_archive_name'], library), file=sys.stderr)\n",
    "        print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "\n",
    "    # Feature beats_position has different length which may cause problems with trained classifiers\n",
    "    # Comment this out if using classifiers trained on features without beats_position feature\n",
    "    \n",
    "    # Load file with correct columns on which classifier was trained\n",
    "    with open('../metadata/misc/columns_fix.json') as f:\n",
    "        cols = json.load(f)\n",
    "\n",
    "    columns = pd.MultiIndex.from_tuples(cols[dataset], names=['feature', 'statistic', 'number'])    \n",
    "    features = features.reindex(columns, fill_value=0, axis=\"columns\")\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "\n",
    "    # Perform One-hot encoding on categorical features\n",
    "    for column in features.select_dtypes(include='object'):   # For each categorical column\n",
    "        dummy_columns = pd.get_dummies(features[column])   # Encode the column values   \n",
    "        features = features.drop(columns=column)   # Drop the column from the dataframe\n",
    "\n",
    "        # Reindex columns to fixed length with all possible instances to avoid feature mismatch with trained classifiers\n",
    "        if (column[0] in ['chords_key', 'key_edma', 'key_krumhansl', 'key_temperley']) and (column[1] in ['none', 'key']):\n",
    "            dummy_columns = dummy_columns.reindex(['Ab', 'B', 'Bb', 'C', 'C#', 'D', 'E', 'Eb', 'F', 'F#', 'G'], axis=1, fill_value=0)\n",
    "        elif (column[0] in ['chords_scale', 'key_edma', 'key_krumhansl', 'key_temperley']) and (column[1] in ['none', 'scale']):\n",
    "            dummy_columns = dummy_columns.reindex(['minor'], axis=1, fill_value=0)\n",
    "\n",
    "        # Create correct multiindex for the encoded columns and append them to the features dataframe\n",
    "        dummy_columns.columns = pd.MultiIndex.from_product([[column[0]], [column[1]], ['{}'.format(c) for c in dummy_columns.columns]], names=features.columns.names)\n",
    "        features = pd.concat([features, dummy_columns], axis=1).sort_index(axis=1)\n",
    "        \n",
    "    # Load track-genre list\n",
    "    try:                \n",
    "        genres = pd.read_csv(\"../metadata/test_data/{}_genres.csv\".format(dataset), index_col=0, header=0)        \n",
    "    except Exception as e:\n",
    "        print('Failed to read file: \"../metadata/test_data/{}_genres.csv\"'.format(dataset), file=sys.stderr)\n",
    "        print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "    \n",
    "    # Fit the encoder to be later able to transform predicted genre values back to actual genres\n",
    "    encoder = LabelEncoder().fit(np.ravel(genres))\n",
    "    \n",
    "    # If optimised feature set is selected, load it, else use all features\n",
    "    if feature_set_name != 'all':\n",
    "        \n",
    "        # Load optimised feature sets if available\n",
    "        try:\n",
    "            with open('../metadata/misc/optimised_feature_sets.json') as f:\n",
    "                optimised_feature_sets = json.load(f)   \n",
    "        except Exception as e:\n",
    "            print('Failed to read file: \"../metadata/misc/optimised_feature_sets.json\"!', file=sys.stderr)\n",
    "            print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "            return -1\n",
    "        \n",
    "        # Remove info about optimisation from classifier name to get correct feature sets key\n",
    "        if '_default' in clf_name:\n",
    "            tmp_clf_name = clf_name.replace('_default', '')\n",
    "        else:\n",
    "            tmp_clf_name = clf_name[:-13]\n",
    "        \n",
    "        feature_set = optimised_feature_sets[dataset][library][tmp_clf_name][feature_set_name]\n",
    "    else:\n",
    "        feature_set = features.columns.levels[0]\n",
    "\n",
    "    scaler = StandardScaler()   # Init the scaler\n",
    "    \n",
    "    X = features[feature_set].values   # Extract feature set values to ndarray\n",
    "    X = scaler.fit_transform(X)   # Scale the features\n",
    "    \n",
    "    # Load selected trained classifier\n",
    "    try:\n",
    "        clf = joblib.load('../metadata/trained_classifiers/{}_{}_{}_{}.joblib.dat'.format(dataset, library, feature_set_name, clf_name))\n",
    "    except Exception as e:\n",
    "        print('Failed to read file: \"../metadata/trained_classifiers/{}_{}_{}_{}.joblib.dat\"'.format(dataset, library, feature_set_name, clf_name), file=sys.stderr)\n",
    "        print('Error: {}'.format(repr(e)), file=sys.stderr)\n",
    "        return -1\n",
    "    \n",
    "    print('Predicting with classifier {} trained on {} dataset:\\n'.format(clf_name, dataset))\n",
    "    \n",
    "    predictions = encoder.inverse_transform(clf.predict(X))   # Transform genre values back to genres\n",
    "    EasyID3.RegisterTextKey('comment', 'COMM')   # Used to set track file ID3 comment\n",
    "    \n",
    "    for track, predicted_genre in dict(zip(list(features.index), predictions)).items():   # For each of the predicted tracks\n",
    "        print('Track: {: <60} Predicted genre: {}'.format(track, predicted_genre))   # Print the track name/path and its predicted genre\n",
    "        \n",
    "        # If annotate tracks is selected, modify tracks ID3 tag\n",
    "        if parameters['annotate_tracks']:\n",
    "            audio = EasyID3(track)\n",
    "            audio['comment'] = 'Genre: {}'.format(predicted_genre)   # Set ID3 comment to be able to see the genre in Nautilus file explorer\n",
    "            audio['genre'] = predicted_genre   # Set the ID3 genre entry\n",
    "            audio.save()   # Save the tag\n",
    "        \n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato buňka spustí predikci žánrů u vybraných skladeb pomocí vybraného natrénovaného klasifikačního algoritmu. <br>\n",
    "ID3 tagy u souborů se skladbami je možné změnit na zákldě predikovaného žánru. <br>\n",
    "Které klasifikátory mají být využity je možné nastavit v proměnné <code>classifiers</code>, název klasifikátoru musí odpovídat názu souboru, ve kterém je natrénovaný klasifikátor uložen. <br>\n",
    "\n",
    "Popis parametrů: <br>\n",
    "\n",
    "<ul>\n",
    "    <li><code>local_archive_name</code> Název archivu se skladbami, pro které má být predikce provedena. (musí odpovídat názvu použitém při extrakci atributů)</li>\n",
    "    <li><code>annotate_tracks</code> Hodnota True značí, že ID3 tag u skladeb bude modifikován, aby obsahoval informaci o predikovaném žánru.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with classifier XGBClassifier_optimised_VS trained on FMA dataset:\n",
      "\n",
      "Track: ../data/local_archive/Electronic_new.mp3                     Predicted genre: Experimental\n",
      "Track: ../data/local_archive/Experimental_new.mp3                   Predicted genre: Experimental\n",
      "Track: ../data/local_archive/Folk_new.mp3                           Predicted genre: Instrumental\n",
      "Track: ../data/local_archive/Hiphop_new.mp3                         Predicted genre: Hip-Hop\n",
      "Track: ../data/local_archive/Instrumental_new.mp3                   Predicted genre: Electronic\n",
      "Track: ../data/local_archive/International_new.mp3                  Predicted genre: International\n",
      "Track: ../data/local_archive/Pop_new.mp3                            Predicted genre: International\n",
      "Track: ../data/local_archive/Rock_new.mp3                           Predicted genre: Rock\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Classifiers to be used for predictions (must be a full name of the saved classifier file)\n",
    "    classifiers = [\n",
    "        'FMA_essentia_all_XGBClassifier_optimised_VS',\n",
    "#         'FMA_essentia_opt_feature_set_FS_VS_MLPClassifier_optimised_VS'\n",
    "    ]\n",
    "    \n",
    "    parameters = {\n",
    "        'local_archive_name': 'local_archive',   # Name of the archive (must match selected name in feature extraction)\n",
    "        'annotate_tracks': True,   # Annotate track files with genre tags\n",
    "    }\n",
    "    \n",
    "    # For each of the selected classifiers perform predictions\n",
    "    for classifier in classifiers:\n",
    "        predict_local(classifier, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
